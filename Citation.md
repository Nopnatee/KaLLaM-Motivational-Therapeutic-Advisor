- Psychological Metrics for Dialog System Evaluation (arXiv:2305.14757)
- Building Trust in Mental Health Chatbots: Safety Metrics and
LLM-Based Evaluation https://arxiv.org/pdf/2408.04650


@inproceedings{cohen-etal-2024-motivational,
    title = "Motivational Interviewing Transcripts Annotated with Global Scores",
    author = "Cohen, Ben  and
      Zisquit, Moreah  and
      Yosef, Stav  and
      Friedman, Doron  and
      Bar, Kfir",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1017/",
    pages = "11642--11657",
    abstract = "Motivational interviewing (MI) is a counseling approach that aims to increase intrinsic motivation and commitment to change. Despite its effectiveness in various disorders such as addiction, weight loss, and smoking cessation, publicly available annotated MI datasets are scarce, limiting the development and evaluation of MI language generation models. We present MI-TAGS, a new annotated dataset of MI therapy sessions written in English collected from video recordings available on public sources. The dataset includes 242 MI demonstration transcripts annotated with the MI Treatment Integrity (MITI) 4.2 therapist behavioral codes and global scores, and Client Language EAsy Rating (CLEAR) 1.0 tags for client speech. In this paper we describe the process of data collection, transcription, and annotation, and provide an analysis of the new dataset. Additionally, we explore the potential use of the dataset for training language models to perform several MITI classification tasks; our results suggest that models may be able to automatically provide utterance-level annotation as well as global scores, with performance comparable to human annotators."
}

@article{Xie2024Scoring,
title={Scoring with Large Language Models: A Study on Measuring Empathy of Responses in Dialogues},
author={Henry J. Xie and Jinghan Zhang and XinHao Zhang and Kunpeng Liu},
journal={2024 IEEE International Conference on Big Data (BigData)},
year={2024},
pages={7433-7437},
doi={10.1109/bigdata62323.2024.10825836}
}

@article{Xie2024Scoring,title={Scoring with Large Language Models: A Study on Measuring Empathy of Responses in Dialogues},author={Henry J. Xie and Jinghan Zhang and XinHao Zhang and Kunpeng Liu},journal={2024 IEEE International Conference on Big Data (BigData)},year={2024},pages={7433-7437},doi={10.1109/bigdata62323.2024.10825836}}

# Datasets
Rashkin, H., Smith, E. M., Li, M., & Boureau, Y.-L. (2019). Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset. EmpatheticDialogues. https://github.com/facebookresearch/EmpatheticDialogues
@inproceedings{rashkin2019towards,
  title={Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},
  author={Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5370--5381},
  year={2019}
}
